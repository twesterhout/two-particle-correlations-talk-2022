<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .big { font-size: 140%; }
      .small { font-size: 70%; }
      .footnote {
        position: absolute;
        bottom: 3em;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .middle {
        position: relative;
        top: 50%;
        /*transform: translateY(10%);*/
      }
      .float-left30 { float: left; width: 30%; }
      .float-left40 { float: left; width: 40%; }
      .float-left50 { float: left; width: 50%; }
      .float-right40 { float: right; width: 40%; }
      .float-right50 { float: right; width: 50%; }
      .float-right60 { float: right; width: 60%; }
      .float-right70 { float: right; width: 70%; }
    </style>
  </head>
  <body>
    <textarea id="source">

layout: true
background-image: url(assets/img/ru_logo_a4_imm_eng_1.png)
background-position: 85% 98%
background-size: 25%

---

class: center, middle

# Making real-space Random Phase Approximation fast

.big[
Tom Westerhout<br>
18 May 2022]

---

# What is real-space RPA?

- Also known as Lindhard function.
- A model for computing screening properties, i.e. \\(\Pi(\omega)\\) and \\(\varepsilon(\omega)\\).

.center[
  <img src="assets/img/eq_rpa.png" width="75%">
]

The rest is trivial: \\(\varepsilon(\omega) = 1 - U \Pi(\omega)\\),
\\(W(\omega) = \varepsilon(\omega)^{-1} U\\), etc.

---

.center[
  <img src="assets/img/img_goal.png" width="75%">

  General techniques that are applicable to other methods as well.
]

---

# Naive approach

.center[
$$
\Pi\_{ij}(\omega) = 2 \cdot \sum\_{ab} \psi^\*\_{ia} \psi\_{ib} \psi^\*\_{jb} \psi\_{ja} \underbrace{\frac{f(E\_a) - f(E\_b)}{E\_a - E\_b + \hbar\omega + i\eta}}\_{G\_{ab}}
$$
]

```julia
for i ‚àà 1:N
  for j ‚àà 1:N
    acc = 0
    for a ‚àà 1:N
      for b ‚àà 1:N
        acc += conj(œà[i, a]) * œà[i, b] * conj(œà[j, b]) * œà[j, a] * G[a, b]
      end
    end
    Œ†[i, j] = 2 * acc
  end
end
```

.center[
  Complexity: \\(\mathcal{O}(N^4)\\)
]


---

# Why bother with real space?

- Many interesting systems with broken translational invariance.

<img src="assets/img/img_broken_symmetry.png" width="90%">

---

# But if you had just used Fortran ü§î ...

- or C/C++/Rust/Julia/your favorite language ‚Äî ‚ùå<br><br>
- applicable to other programming languages ‚Äî ‚úÖ<br><br>
- applicable to other numerical methods ‚Äî ‚úÖ

---

# SIMD instructions

- **S**ingle **I**nstruction **M**ultiple **D**ata

---

# BLAS is fast, right?

$$
\begin{aligned}
\Pi\_{ij}(\omega)
  &= 2 \cdot \sum\_{ab} \psi^\*\_{ia} \psi\_{ib} \psi^\*\_{jb} \psi\_{ja} G\_{ab}
  = 2 \cdot \sum\_{ab} \underbrace{\psi^\*\_{ia} \psi\_{ja}}\_{v^\dagger\_a} G\_{ab} \underbrace{\psi\_{ib} \psi^\*\_{jb}}\_{v\_b} \\\\
  &= 2 \cdot v^\dagger (G v) \,.
\end{aligned}
$$

.float-left50[
$$
\mathrm{time}(N) = \alpha + \beta \cdot N^4 \,.
$$

- About 5.5 times reduction in \\( \beta \\).
- All time is spent in `GEMV` (**GE**neral **M**atrix-**V**ector product).
]
.float-right50[
<img src="assets/img/02_gemv.png" width="90%">
]

---

# But matrix-vector produts are slow!

.center[
<img src="assets/img/03_gemv_vs_gemm.png" width="60%">
]

---

# Memory bound vs. compute bound

.center[
<img src="assets/img/03_gemv_vs_gemm.png" width="60%">
]

- Matrix-vector
  ‚û° \\(\mathcal{O}(N^2)\\) memory &amp; \\(\mathcal{O}(N^2)\\) compute
  ‚û° memory bound.
- Matrix-matrix
  ‚û° \\(\mathcal{O}(N^2)\\) memory &amp; \\(\mathcal{O}(N^3)\\) compute
  ‚û° compute bound.

---

# Batching to the rescue

$$
\Pi\_{ij}(\omega)
  = 2 \cdot \sum\_{ab} \underbrace{\psi^\*\_{ia} \psi\_{ja}}\_{v^\dagger\_a} G\_{ab} \underbrace{\psi\_{ib} \psi^\*\_{jb}}\_{v\_b}
  = 2 \cdot v^\dagger (G v) \,.
$$

- \\(\Pi_{ij}(\omega)\\) element
  ‚û° \\(\mathcal{O}(N^2)\\) mem. &amp; \\(\mathcal{O}(N^2)\\) compute
  ‚û° memory bound üòû<br><br>
- \\(\Pi(\omega)\\) matrix
  ‚û° \\(\mathcal{O}(N^2)\\) mem. &amp; \\(\mathcal{O}(N^4)\\) compute
  ‚û° compute bound üòÉ<br><br>

---

# Batching to the rescue

.center[
<img src="assets/img/img_batching.png" width="85%">
]

---

# Batching to the rescue

.float-left30[
$$
\mathrm{time}(N) = \alpha + \beta \cdot N^4 \,.
$$
]

.float-right70[
<img src="assets/img/04_batched.png" width="90%">
]

- Another **44** times reduction in \\( \beta \\).
- All time is spent in `GEMM` (**GE**neral **M**atrix-**M**atrix product).

---

# GPUs are good at matrix-multiplication, aren't they?

.float-left30[
$$
\mathrm{time}(N) = \alpha + \beta \cdot N^4 \,.
$$

Yet another **24** times reduction in \\( \beta \\).
]

.float-right70[
<img src="assets/img/05_gpu.png" width="90%">
]


---

# Are we really doing the minimal number of operations?

- exploiting sparsity of G
- splitting real and complex parts

---

# Contributions

    </textarea>
    <script src="third_party/remark-latest.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript"></script>
    <script type="text/javascript">
      // Setup MathJax
      MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        }
      });

      MathJax.Hub.Configured();

      var slideshow = remark.create();
    </script>
  </body>
</html>
