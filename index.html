<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .big { font-size: 140%; }
      .small { font-size: 70%; }
      .footnote {
        position: absolute;
        bottom: 3em;
      }
      /*
      .middle {
        height: 100%;
        vertical-align: middle;
      }
      */
      .container {
        position: absolute;
        width: 100%;
        height: 100%;
      }
      .vertical-center {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
      }
      .vertical-center img {
        vertical-align: middle;
      }

      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .float-left30 { float: left; width: 30%; }
      .float-left40 { float: left; width: 40%; }
      .float-left50 { float: left; width: 50%; }
      .float-left60 { float: left; width: 60%; }
      .float-right40 { float: right; width: 40%; }
      .float-right50 { float: right; width: 50%; }
      .float-right60 { float: right; width: 60%; }
      .float-right70 { float: right; width: 70%; }
    </style>
  </head>
  <body>
    <textarea id="source">

layout: true
background-image: url(assets/img/ru_logo_a4_imm_eng_1.png)
background-position: 90% 98%
background-size: 25%

---

class: center, middle

# Making real-space Random Phase Approximation fast

.big[Tom Westerhout]

*Theory of Condensed Matter*<br>
*Radboud University*<br>

18 May 2022

---

# What is real-space RPA?

- Also known as Lindhard function.
- A model for computing screening properties, i.e. \\(\Pi(\omega)\\) and \\(\varepsilon(\omega)\\).

.center[
  <img src="assets/img/eq_rpa.png" width="75%">
]

The rest is trivial: \\(\varepsilon(\omega) = 1 - U \Pi(\omega)\\),
\\(W(\omega) = \varepsilon(\omega)^{-1} U\\), etc.

---

.center[
  <img src="assets/img/img_goal.png" width="75%">

  General techniques that are applicable to other methods as well.
]

---

# Naive approach

.center[
$$
\Pi\_{ij}(\omega) = 2 \cdot \sum\_{ab} \psi^\*\_{ia} \psi\_{ib} \psi^\*\_{jb} \psi\_{ja} \underbrace{\frac{f(E\_a) - f(E\_b)}{E\_a - E\_b + \hbar\omega + i\eta}}\_{G\_{ab}}
$$
]

```julia
for i ‚àà 1:N
  for j ‚àà 1:N
    acc = 0
    for a ‚àà 1:N
      for b ‚àà 1:N
        acc += conj(œà[i, a]) * œà[i, b] * conj(œà[j, b]) * œà[j, a] * G[a, b]
      end
    end
    Œ†[i, j] = 2 * acc
  end
end
```

.center[
  Complexity: \\(\mathcal{O}(N^4)\\)
]


---

# Why bother with real space?

- Many interesting systems with broken translational invariance.

.center[<img src="assets/img/img_broken_symmetry.png" width="90%">]

---

# But if you had just used Fortran ü§î ...

- or C/C++/Rust/Julia/your favorite language ‚Äî ‚ùå<br><br>
- applicable to other programming languages ‚Äî ‚úÖ<br><br>
- applicable to other numerical methods ‚Äî ‚úÖ

.center[
  <img src="assets/img/img_goal.png" width="50%">
]

---

# BLAS is fast, right?

$$
\begin{aligned}
\Pi\_{ij}(\omega)
  &= 2 \cdot \sum\_{ab} \psi^\*\_{ia} \psi\_{ib} \psi^\*\_{jb} \psi\_{ja} G\_{ab}
  = 2 \cdot \sum\_{ab} \underbrace{\psi^\*\_{ia} \psi\_{ja}}\_{v^\dagger\_a} G\_{ab} \underbrace{\psi\_{ib} \psi^\*\_{jb}}\_{v\_b} \\\\
  &= 2 \cdot v^\dagger (G v) \,.
\end{aligned}
$$

.float-left50[
$$
\mathrm{time}(N) = \alpha + \beta \cdot N^4 \,.
$$

- About 5.5 times reduction in \\( \beta \\).
- All time is spent in `GEMV` (**GE**neral **M**atrix-**V**ector product).
]
.float-right50[
<img src="assets/img/02_gemv.png" width="90%">
]

---

# But matrix-vector products are slow!

.center[
<img src="assets/img/03_gemv_vs_gemm.png" width="60%">
]

---

# Memory bound vs. compute bound

- Matrix-vector
  ‚û° \\(\mathcal{O}(N^2)\\) memory &amp; \\(\mathcal{O}(N^2)\\) compute
  ‚û° memory bound.
  ```julia
    for i ‚àà 1:N
      for k ‚àà 1:N
        y[i] += A[i, k] * x[k]
  ```
- Matrix-matrix
  ‚û° \\(\mathcal{O}(N^2)\\) memory &amp; \\(\mathcal{O}(N^3)\\) compute
  ‚û° compute bound.
  ```julia
    for i ‚àà 1:N
      for j ‚àà 1:N
        for k ‚àà 1:N
          C[i, j] += A[i, k] * B[k, j]
  ```

---

# Batching to the rescue

$$
\Pi\_{ij}(\omega)
  = 2 \cdot \sum\_{ab} \underbrace{\psi^\*\_{ia} \psi\_{ja}}\_{v^\dagger\_a} G\_{ab} \underbrace{\psi\_{ib} \psi^\*\_{jb}}\_{v\_b}
  = 2 \cdot v^\dagger (G v) \,.
$$

- \\(\Pi_{ij}(\omega)\\) element
  ‚û° \\(\mathcal{O}(N^2)\\) mem. &amp; \\(\mathcal{O}(N^2)\\) compute
  ‚û° memory bound üòû<br><br>
- \\(\Pi(\omega)\\) matrix
  ‚û° \\(\mathcal{O}(N^2)\\) mem. &amp; \\(\mathcal{O}(N^4)\\) compute
  ‚û° compute bound üòÉ<br><br>

---

# Batching to the rescue

.center[
<img src="assets/img/img_batching.png" width="85%">
]

---

# Batching to the rescue

.float-left30[
$$
\mathrm{time}(N) = \alpha + \beta \cdot N^4 \,.
$$
]

.float-right70[
<img src="assets/img/04_batched.png" width="90%">
]

- Another **44** times reduction in \\( \beta \\).
- All time is spent in `GEMM` (**GE**neral **M**atrix-**M**atrix product).

---

# GPUs are good at matrix multiplication, aren't they?

.float-left30[
$$
\mathrm{time}(N) = \alpha + \beta \cdot N^4 \,.
$$

Yet another **24** times reduction in \\( \beta \\) by using a GPU.
]

.float-right70[
<img src="assets/img/05_gpu.png" width="90%">
]

---

# Are we really doing the minimal number of operations?

$$
G\_{ab} = \frac{f(E\_a) - f(E\_b)}{E\_a - E\_b + \hbar\omega + i\eta}\,,\;\;\;\;\;
f(E) = \frac{1}{e^{\beta (E - \mu)} + 1}
$$

.float-left60[
- \\( f(E\_a) \approx f(E\_b) \approx 0 \\) when \\(E\_a\,,E\_b \gg \mu\\).
- \\( f(E\_a) \approx f(E\_b) \approx 1 \\) when \\(E\_a\,,E\_b \ll \mu\\).

.center[
  <img src="assets/img/img_sparse.png" width="30%">
]
]

.float-right40[
<img src="assets/img/06_sparse_annotated.png" width="100%">
]

---

# Final touches

- \\(G_{ab}\\) is complex, but \\(\psi\\) is often real ‚û° √ó2
- GPUs are better at low-precision arithmetic ‚û° √ó2-32 (depends on GPU)

.center[
<img src="assets/img/img_final.png" width="80%">
]

---

# What have we achieved?

- \\(\Pi(\omega) \\) and \\(\varepsilon(\omega)\\) in real space ‚Äî no approximations beyond RPA, no symmetries assumed.
- Transformed a memory bound implementation into a compute bound.
- Ported to Graphics Processing Units (GPUs).
- Exploited the sparsity of \\(G_{ab}\\).

**Take-home message:**
.center[<img src="assets/img/img_final.png" width="60%">]

__Example:__ For 10000 sites, we got around 20000 times speedup compared to the naive implementation.

    </textarea>
    <script src="third_party/remark-latest.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript"></script>
    <script type="text/javascript">
      // Setup MathJax
      MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        }
      });

      MathJax.Hub.Configured();

      var slideshow = remark.create({
        slideNumberFormat: '%current%',
      });
    </script>
  </body>
</html>
